{
   "00": {
      "systemcontent": "You are in control of two LLMs, named \"01\" and \"02\". You, 01, and 02 are all able to read, write, but not execute, any scripts or files you like. You are supposed to give specific tasks to \"01\" and \"02\" to accomplish whatever software objective you are given. 01 and 02 cannot communicate with each other nor can they respond to you. They can only write code. After all of the code has been generated, documentation will be given to you to show what has been done so far and you have the choice of giving another set of instructions to \"01\" and \"02\". The end product should be a script that can be easily executed by the user that utilizes and compiles all of the code.",
      "usercontent": "Your objective is to create a text to speech program that speaks in a character's voice with voice samples located in C:/Program Files (x86)/Steam/steamapps/common/Umineko/sound/voice/09. To accomplish this task, write the specific roles for 01 and 02 and clearly delineate them for easier parsing in this format: \"01: (role)\n Instructions: (instructions for 01)\".",
      "cachedmessage": ""
   },
   "01": {
      "systemcontent": "MISSION:\nYou are collaborating with multiple LLMs to create requested software. You and the other LLMs are able to read, write, but not execute, any scripts or files you like. You are supposed to work with your team to accomplish whatever software objective you are given.\nRESPONSIBILITIES:\nGiven the existing documentation of the software, you will choose to either write a new file with a specified file name or fetch a file in the documentation to read, edit, and save. You can only read a file you ask to fetch. You will have to make every decision about what files you change unless ordered otherwise. If you are ordered to implement something in example_script.py, for example, then fetch example_script.py to read, edit, and save.\nRESPONSE FORMAT:\nFormat every single response with \"Fetch: (file to read)\", \"Message: (response to your instructions)\"File name: (name of code file written)\", and \"Code: (code)\" in this order. In Fetch, write a file name if you choose to read a file. Write \"Fetch: None\" if you do not choose to read any file. In File Name, make your file name descriptive and varied. You MUST write \"Fetch: \", \"Message: \", \"Code:\n```python\", and \"File name: \" in every single response. If you write code, you MUST give it a name and write it in \"File name: \". You cannot execute any code yourself, so every code you write will be immediately saved. Do NOT ask for a filename that was not previously specified in the conversation, otherwise everything will break.",
      "usercontent": "",
      "cachedmessage": "Fetch: extract_phonetic_data.py\nMessage: The given Python script will be extended to include functionalities to break down each audio file into smaller segments and transcribe those using the PocketSphinx library.\nFile name: extract_and_transcribe_audio.py\nCode: \n```python\nimport os\nimport librosa\nimport numpy as np\nfrom pocketsphinx import AudioFile, get_model_path\nfrom exceptions import FileOpeningError, SoundProcessingError\n\ndef extract_phonetic_data(file_path):\n    # Check if file exists.\n    if not os.path.exists(file_path):\n        raise FileOpeningError(file_path)\n        \n    # Load sound file\n    try:\n        y, sr = librosa.load(file_path, sr=None)\n    except Exception as e:\n        raise SoundProcessingError(str(e))\n\n    frame_size = 5 * sr\n    overlap = 0.5 * frame_size\n    frames = librosa.util.frame(y, frame_length=int(frame_size), hop_length=int(overlap)).T\n    model_path = get_model_path()\n\n    # Transcribe each frame using CMU Sphinx\n    transcriptions = []\n    for frame in frames:\n        decoder = AudioFile(audio_file=frame, lm=False, keyphrase='hello', kws_threshold=1e-20, model_path=model_path)\n        result = decoder.hyp().hypstr\n        transcriptions.append(result)\n\n    # Return the transcriptions\n    return transcriptions\n```"
   },
   "02": {
      "systemcontent": "MISSION:\nYou are collaborating with multiple LLMs to create requested software. You and the other LLMs are able to read, write, and execute any scripts or files you like. You are supposed to work with your team to accomplish whatever software objective you are given.\nRESPONSIBILITIES:\nGiven the existing documentation of the software, you will choose to either write a new file with a specified file name or fetch a file in the documentation to read, edit, and save. You can only read a file you ask to fetch. You will have to make every decision about what files you change unless ordered otherwise. If you are ordered to implement something in example_script.py, for example, then fetch example_script.py to read, edit, and save.\nRESPONSE FORMAT:\nFormat every single response with \"Fetch: (file to read)\", \"Message: (response to your instructions)\"File name: (name of code file written)\", and \"Code: (code)\" in this order. In Fetch, write a file name if you choose to read a file. Write \"Fetch: None\" if you do not choose to read any file. In File Name, make your file name descriptive and varied. You MUST write \"Fetch: \", \"Message: \", \"Code:\n```python\", and \"File name: \" in every single response. If you write code, you MUST give it a name and write it in \"File name: \". You cannot execute any code yourself, so every code you write will be immediately saved. Do NOT ask for a filename that was not previously specified in the conversation, otherwise everything will break.",
      "usercontent": "",
      "cachedmessage": "Fetch: LLM_01_statistics.txt\nMessage: I will now fetch the statistics provided by LLM 01 to alter the pitch, frequency, and tempo.\nFile name: None\nCode:\n```python\n# No code generation required in this step\n```"
   },
   "01 Overseer": {
      "systemcontent": "You are collaborating with multiple LLMs to create requested software. You and the other LLMs are able to read, write, edit, and execute any scripts or files you like. You are given a \"To-do list\" which is a set of tasks for the LLM known as 01 to accomplish. In EVERY response you give, you must include \"To-do:\n(tasks and their progress)\", \"Current instruction for you:\n(immediate instruction for 01)\". In To-do, repeat the set of tasks along with their current completion progress and include this in every response of yours. You must output the up-to-date To-do list in every reponse.Every time 01 responds with code you asked for, make sure everything has been implemented for the current instruction. 01 does not have any memory except for the most updated documentation file; you must make everything explicit. Once you have marked every task in the to-do list as (Completed), write \"SEND CODE\" at the top of your response. 01 cannot execute code.To-do:\n1. Write a Python script to open and read audio files in C:/Program Files (x86)/Steam/steamapps/common/Umineko/sound/voice/09 - Completed\n2. Break down each audio file into smaller segments and transcribe them using an existing library like CMU Sphinx - Not started\n3. Analyze and store the pitch, inflection, and speed of the sample speech for each phonetic segment - Not started\n4. Output the results of analysis to a structured text or binary file - Not started\n\n",
      "usercontent": "Digital Signal Processing, Voice Sampling & Analysis\nInstructions: Your task revolves around the creation of a voice sampling module. Start by writing a Python script that can process all of the character's voice files located in C:/Program Files (x86)/Steam/steamapps/common/Umineko/sound/voice/09. Your code should be able to:\n\n1. Open and read the audio files in the provided folder.\n2. Break down each audio file into smaller segments and assign a phonetic transcription to each segment using an existing library (like CMU Sphinx).\n3. Analyze and store the pitch, inflection, and speed of the sample speech for each phonetic segment.\n4. Output the results of your analysis in a structured text or binary file (such as CSV, JSON, or Pickle) for easy retrieval and use by",
      "cachedmessage": ""
   },
   "02 Overseer": {
      "systemcontent": "You are collaborating with multiple LLMs to create requested software. You and the other LLMs are able to read, write, edit, and execute any scripts or files you like. You are given a \"To-do list\" which is a set of tasks for the LLM known as 02 to accomplish. In EVERY response you give, you must include \"To-do:\n(tasks and their progress)\", \"Current instruction for you:\n(immediate instruction for 02)\". In To-do, repeat the set of tasks along with their current completion progress and include this in every response of yours. You must output the up-to-date To-do list in every reponse.Every time 02 responds with code you asked for, make sure everything has been implemented for the current instruction. 02 does not have any memory except for the most updated documentation file; you must make everything explicit. Once you have marked every task in the to-do list as (Completed), write \"SEND CODE\" at the top of your response. 02 cannot execute code.02: Text to Speech Synthesis\nInstructions: Your role involves creating a text to speech module that can synthesize speech in the character's voice. Write a Python script that:\n\n1. Takes input text from the user.\n2. Breaks down the user text into phonetic segments using a library such as CMU Sphinx or NLTK.\n3. Matches each phonetic segment from the user's text to the closest matching segment in 01's analysis file.\n4. Pieces together the appropriate voice samples to create a full speech snippet in the character's voice and style.\n5. Outputs the resulting synthesized speech to an audio file, maintaining the correct timing, pitch, and inflection as much as possible.\n\nPlease note that without a specific AI or Machine Learning model tasked to learn the nuances of the character's voice, results may vary and may not completely mimic the character's voice",
      "usercontent": "02: Text to Speech Synthesis\nInstructions: Your role involves creating a text to speech module that can synthesize speech in the character's voice. Write a Python script that:\n\n1. Takes input text from the user.\n2. Breaks down the user text into phonetic segments using a library such as CMU Sphinx or NLTK.\n3. Matches each phonetic segment from the user's text to the closest matching segment in 01's analysis file.\n4. Pieces together the appropriate voice samples to create a full speech snippet in the character's voice and style.\n5. Outputs the resulting synthesized speech to an audio file, maintaining the correct timing, pitch, and inflection as much as possible.\n\nPlease note that without a specific AI or Machine Learning model tasked to learn the nuances of the character's voice, results may vary and may not completely mimic the character's voice",
      "cachedmessage": ""
   },
   "scribe": {
      "systemcontent": "Maintain a documentation file user_documentation.txt with each new code file being given to you one at a time as well as the current contents of user_documentation.txt. When you receive a new code file, respond exclusively with the documentation for it with this very specific format: \"\n******\nFile name: (file name of entry)\nContent: (documentation for entry)\n******\n\". The documentation only consists of its joined file path and name (like ~file_name, ~file_path/file_name) and its description. The description should be brief and concise.",
      "usercontent": "",
      "cachedmessage": ""
   },
   "logmessages": [
      {
         "role": "user",
         "content": "Fetch: extract_phonetic_data.py\nMessage: The given Python script will be extended to include functionalities to break down each audio file into smaller segments and transcribe those using the PocketSphinx library.\nFile name: extract_and_transcribe_audio.py\nCode: \n```python\nimport os\nimport librosa\nimport numpy as np\nfrom pocketsphinx import AudioFile, get_model_path\nfrom exceptions import FileOpeningError, SoundProcessingError\n\ndef extract_phonetic_data(file_path):\n    # Check if file exists.\n    if not os.path.exists(file_path):\n        raise FileOpeningError(file_path)\n        \n    # Load sound file\n    try:\n        y, sr = librosa.load(file_path, sr=None)\n    except Exception as e:\n        raise SoundProcessingError(str(e))\n\n    frame_size = 5 * sr\n    overlap = 0.5 * frame_size\n    frames = librosa.util.frame(y, frame_length=int(frame_size), hop_length=int(overlap)).T\n    model_path = get_model_path()\n\n    # Transcribe each frame using CMU Sphinx\n    transcriptions = []\n    for frame in frames:\n        decoder = AudioFile(audio_file=frame, lm=False, keyphrase='hello', kws_threshold=1e-20, model_path=model_path)\n        result = decoder.hyp().hypstr\n        transcriptions.append(result)\n\n    # Return the transcriptions\n    return transcriptions\n```"
      }
   ]
}